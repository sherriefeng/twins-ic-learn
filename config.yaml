### ----- Begin Common Configurables ----- ###

defaults:
  # Meta: Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct
  # Microsoft: Phi-3-mini-128k-instruct, Phi-3-medium-128k-instruct
  # Alphabet: Google-Gemma-2-27b-it
  - model_configs:  Meta-Llama-3.1-8B-Instruct

# how many hours to keep the API server running for (increasing this will dramatically increase wait time)
time: 0.5

# where the fail emails should go (otherwise Venia gets them!)
# model_configs:
#   mail_user: null@null.null

# to run bigger models we will require to use the pli cluster
use_pli: False # set to True if you want to use the pli cluster
# use_pli_partition: pli
# use_pli_account: False # for backwards compatibility; leave False if you are running as yourself

### ----- End Common Configurables ----- ###

### DO NOT CHANGE THE BELOW UNLESS YOU KNOW WHAT YOU ARE DOING ###
# determine if you want to run it on Azure (changes slurm)
use_azure: False
azure_user: venia # NOTE: !!! add your azure user !!!
# most likely ignore (edit only if you change templates)
slurm_dir: templates
slurm_fname: slurm_template.txt
slurm_azure_fname: slurm_azure_template.txt
